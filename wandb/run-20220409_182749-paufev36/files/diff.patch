diff --git a/__pycache__/barlow.cpython-37.pyc b/__pycache__/barlow.cpython-37.pyc
index d9b3757..420c21a 100644
Binary files a/__pycache__/barlow.cpython-37.pyc and b/__pycache__/barlow.cpython-37.pyc differ
diff --git a/__pycache__/train_translation.cpython-37.pyc b/__pycache__/train_translation.cpython-37.pyc
index 7bf3ea7..b5b1fb5 100644
Binary files a/__pycache__/train_translation.cpython-37.pyc and b/__pycache__/train_translation.cpython-37.pyc differ
diff --git a/barlow.py b/barlow.py
index 99b0da9..b20d671 100644
--- a/barlow.py
+++ b/barlow.py
@@ -265,13 +265,6 @@ def main_worker(gpu, args):
                          optimizer=optimizer.state_dict())
             torch.save(state, args.checkpoint_dir / 'barlow_checkpoint.pth')
             print('barlow model saved in', args.checkpoint_dir)
-            for sent in test_loader: 
-                y1 = sent[0].cuda(gpu, non_blocking=True)
-                y2 = sent[1].cuda(gpu, non_blocking=True)
-                model.eval()
-                c, _ = model(y1, y2)
-                xlabels = tokenizer.convert_ids_to_tokens(y2)
-                ylabels = tokenizer.convert_ids_to_tokens(y1)
 #    wandb.finish()
 #    if args.rank == 0:
 #        save final model
diff --git a/checkpoint/stats.txt b/checkpoint/stats.txt
index 97f9eb6..e8bd4e3 100644
--- a/checkpoint/stats.txt
+++ b/checkpoint/stats.txt
@@ -467,3 +467,362 @@ train_translation.py
 {"epoch": 4, "step": 210, "loss": 173.08377075195312, "time": 132}
 {"epoch": 4, "step": 215, "loss": 95.62724304199219, "time": 133}
 {"epoch": 4, "step": 220, "loss": 146.6149444580078, "time": 133}
+/home/ivlabs/context_enhancement/context_enhancement/train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 5}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 5}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 6}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 7}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 7}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 8}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 8}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 9}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 9}
+{"epoch": 1, "step": 45, "loss": 83.40009307861328, "time": 60}
+{"epoch": 1, "step": 50, "loss": 83.65938568115234, "time": 61}
+{"epoch": 1, "step": 55, "loss": 121.91883850097656, "time": 61}
+{"epoch": 1, "step": 60, "loss": 69.18376159667969, "time": 62}
+{"epoch": 1, "step": 65, "loss": 108.74915313720703, "time": 63}
+{"epoch": 1, "step": 70, "loss": 72.30828094482422, "time": 63}
+{"epoch": 1, "step": 75, "loss": 71.81124114990234, "time": 64}
+{"epoch": 1, "step": 80, "loss": 68.34536743164062, "time": 64}
+{"epoch": 1, "step": 85, "loss": 60.85449981689453, "time": 65}
+{"epoch": 2, "step": 90, "loss": 77.74386596679688, "time": 85}
+{"epoch": 2, "step": 95, "loss": 96.72307586669922, "time": 85}
+{"epoch": 2, "step": 100, "loss": 101.80294036865234, "time": 86}
+{"epoch": 2, "step": 105, "loss": 84.51009368896484, "time": 86}
+{"epoch": 2, "step": 110, "loss": 72.72525787353516, "time": 87}
+{"epoch": 2, "step": 115, "loss": 74.45042419433594, "time": 88}
+{"epoch": 2, "step": 120, "loss": 67.41654968261719, "time": 88}
+{"epoch": 2, "step": 125, "loss": 78.1681137084961, "time": 89}
+{"epoch": 2, "step": 130, "loss": 92.35138702392578, "time": 89}
+{"epoch": 3, "step": 135, "loss": 67.62174224853516, "time": 106}
+{"epoch": 3, "step": 140, "loss": 73.0427017211914, "time": 106}
+{"epoch": 3, "step": 145, "loss": 105.50846099853516, "time": 107}
+{"epoch": 3, "step": 150, "loss": 80.58209991455078, "time": 108}
+{"epoch": 3, "step": 155, "loss": 93.44019317626953, "time": 108}
+{"epoch": 3, "step": 160, "loss": 89.55480194091797, "time": 109}
+{"epoch": 3, "step": 165, "loss": 105.64498138427734, "time": 109}
+{"epoch": 3, "step": 170, "loss": 114.21644592285156, "time": 110}
+{"epoch": 3, "step": 175, "loss": 132.64865112304688, "time": 110}
+{"epoch": 4, "step": 180, "loss": 123.47101593017578, "time": 129}
+{"epoch": 4, "step": 185, "loss": 98.48711395263672, "time": 130}
+{"epoch": 4, "step": 190, "loss": 106.57389831542969, "time": 130}
+{"epoch": 4, "step": 195, "loss": 123.41980743408203, "time": 131}
+{"epoch": 4, "step": 200, "loss": 133.0455322265625, "time": 131}
+{"epoch": 4, "step": 205, "loss": 115.12477111816406, "time": 132}
+{"epoch": 4, "step": 210, "loss": 173.08377075195312, "time": 132}
+{"epoch": 4, "step": 215, "loss": 95.62724304199219, "time": 133}
+{"epoch": 4, "step": 220, "loss": 146.6149444580078, "time": 134}
+/home/ivlabs/context_enhancement/context_enhancement/barlow.py --load 1
+/home/ivlabs/context_enhancement/context_enhancement/train_translation.py --load 1
+/home/ivlabs/context_enhancement/context_enhancement/train_translation.py --load 0
+train_translation.py --batch_size=32 --dfeedforward=1024 --epochs=28 --nhead=8 --nlayers=6
+{"epoch": 0, "step": 0, "loss": 7.120170593261719, "time": 5}
+{"epoch": 0, "step": 5, "loss": 151.9119415283203, "time": 8}
+{"epoch": 1, "step": 10, "loss": 112.8124008178711, "time": 84}
+{"epoch": 2, "step": 15, "loss": 47.12509536743164, "time": 111}
+{"epoch": 3, "step": 20, "loss": 45.04984664916992, "time": 139}
+{"epoch": 4, "step": 25, "loss": 38.9657096862793, "time": 165}
+{"epoch": 5, "step": 30, "loss": 60.226715087890625, "time": 190}
+{"epoch": 5, "step": 35, "loss": 65.24925231933594, "time": 192}
+{"epoch": 6, "step": 40, "loss": 65.57554626464844, "time": 268}
+{"epoch": 7, "step": 45, "loss": 61.62765121459961, "time": 294}
+{"epoch": 8, "step": 50, "loss": 64.9477310180664, "time": 319}
+{"epoch": 9, "step": 55, "loss": 72.8912353515625, "time": 344}
+{"epoch": 10, "step": 60, "loss": 86.97362518310547, "time": 369}
+{"epoch": 10, "step": 65, "loss": 112.7873306274414, "time": 372}
+{"epoch": 11, "step": 70, "loss": 88.19213104248047, "time": 447}
+{"epoch": 12, "step": 75, "loss": 73.24372863769531, "time": 472}
+{"epoch": 13, "step": 80, "loss": 73.8764419555664, "time": 498}
+{"epoch": 14, "step": 85, "loss": 87.44139099121094, "time": 525}
+{"epoch": 15, "step": 90, "loss": 66.60698699951172, "time": 551}
+{"epoch": 15, "step": 95, "loss": 80.11738586425781, "time": 553}
+{"epoch": 16, "step": 100, "loss": 88.93124389648438, "time": 624}
+{"epoch": 17, "step": 105, "loss": 74.59225463867188, "time": 649}
+{"epoch": 18, "step": 110, "loss": 108.9293441772461, "time": 675}
+{"epoch": 19, "step": 115, "loss": 87.63671112060547, "time": 700}
+{"epoch": 20, "step": 120, "loss": 99.23358154296875, "time": 725}
+{"epoch": 20, "step": 125, "loss": 118.16622924804688, "time": 727}
+{"epoch": 21, "step": 130, "loss": 102.9515380859375, "time": 801}
+{"epoch": 22, "step": 135, "loss": 80.40345764160156, "time": 827}
+{"epoch": 23, "step": 140, "loss": 87.99221801757812, "time": 852}
+{"epoch": 24, "step": 145, "loss": 63.2794303894043, "time": 876}
+{"epoch": 25, "step": 150, "loss": 78.17864227294922, "time": 902}
+{"epoch": 25, "step": 155, "loss": 100.8608169555664, "time": 904}
+{"epoch": 26, "step": 160, "loss": 88.68865203857422, "time": 976}
+{"epoch": 27, "step": 165, "loss": 84.6174087524414, "time": 1002}
+train_translation.py --batch_size=256 --dfeedforward=512 --epochs=32 --nhead=6 --nlayers=4
+{"epoch": 0, "step": 0, "loss": 7.139744758605957, "time": 6}
+train_translation.py --batch_size=128 --dfeedforward=1024 --epochs=36 --nhead=4 --nlayers=4
+{"epoch": 0, "step": 0, "loss": 7.140841484069824, "time": 6}
+train_translation.py --batch_size=16 --dfeedforward=1024 --epochs=32 --nhead=6 --nlayers=2
+{"epoch": 0, "step": 0, "loss": 7.180241584777832, "time": 5}
+train_translation.py --batch_size=128 --dfeedforward=1024 --epochs=20 --nhead=8 --nlayers=6
+{"epoch": 0, "step": 0, "loss": 7.120020389556885, "time": 6}
+train_translation.py --batch_size=64 --dfeedforward=512 --epochs=32 --nhead=2 --nlayers=6
+{"epoch": 0, "step": 0, "loss": 7.082856178283691, "time": 6}
+train_translation.py --batch_size=128 --dfeedforward=512 --epochs=16 --nhead=6 --nlayers=4
+{"epoch": 0, "step": 0, "loss": 7.140233993530273, "time": 6}
+train_translation.py --batch_size=256 --dfeedforward=256 --epochs=40 --nhead=6 --nlayers=2
+train_translation.py
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 5}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 4}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 4}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 5}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 5}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 6}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 6}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 6}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 4}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 4}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 5}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 5}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 6}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 6}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 6}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 4}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 4}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 5}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 5}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 6}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 6}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 6}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 4}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 4}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 5}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 5}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 6}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 6}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 6}
+{"epoch": 1, "step": 45, "loss": 83.40009307861328, "time": 54}
+{"epoch": 1, "step": 50, "loss": 83.65938568115234, "time": 54}
+{"epoch": 1, "step": 55, "loss": 121.91883850097656, "time": 54}
+{"epoch": 1, "step": 60, "loss": 69.18376159667969, "time": 55}
+{"epoch": 1, "step": 65, "loss": 108.74915313720703, "time": 55}
+{"epoch": 1, "step": 70, "loss": 72.30828094482422, "time": 55}
+{"epoch": 1, "step": 75, "loss": 71.81124114990234, "time": 56}
+{"epoch": 1, "step": 80, "loss": 68.34536743164062, "time": 56}
+{"epoch": 1, "step": 85, "loss": 60.85449981689453, "time": 56}
+{"epoch": 2, "step": 90, "loss": 77.74386596679688, "time": 77}
+{"epoch": 2, "step": 95, "loss": 96.72307586669922, "time": 77}
+{"epoch": 2, "step": 100, "loss": 101.80294036865234, "time": 77}
+{"epoch": 2, "step": 105, "loss": 84.51009368896484, "time": 78}
+{"epoch": 2, "step": 110, "loss": 72.72525787353516, "time": 78}
+{"epoch": 2, "step": 115, "loss": 74.45042419433594, "time": 78}
+{"epoch": 2, "step": 120, "loss": 67.41654968261719, "time": 79}
+{"epoch": 2, "step": 125, "loss": 78.1681137084961, "time": 79}
+{"epoch": 2, "step": 130, "loss": 92.35138702392578, "time": 79}
+{"epoch": 3, "step": 135, "loss": 67.62174224853516, "time": 97}
+{"epoch": 3, "step": 140, "loss": 73.0427017211914, "time": 97}
+{"epoch": 3, "step": 145, "loss": 105.50846099853516, "time": 98}
+{"epoch": 3, "step": 150, "loss": 80.58209991455078, "time": 98}
+{"epoch": 3, "step": 155, "loss": 93.44019317626953, "time": 98}
+{"epoch": 3, "step": 160, "loss": 89.55480194091797, "time": 99}
+{"epoch": 3, "step": 165, "loss": 105.64498138427734, "time": 99}
+{"epoch": 3, "step": 170, "loss": 114.21644592285156, "time": 99}
+{"epoch": 3, "step": 175, "loss": 132.64865112304688, "time": 100}
+{"epoch": 4, "step": 180, "loss": 123.47101593017578, "time": 116}
+{"epoch": 4, "step": 185, "loss": 98.48711395263672, "time": 117}
+{"epoch": 4, "step": 190, "loss": 106.57389831542969, "time": 117}
+{"epoch": 4, "step": 195, "loss": 123.41980743408203, "time": 118}
+{"epoch": 4, "step": 200, "loss": 133.0455322265625, "time": 118}
+{"epoch": 4, "step": 205, "loss": 115.12477111816406, "time": 118}
+{"epoch": 4, "step": 210, "loss": 173.08377075195312, "time": 119}
+{"epoch": 4, "step": 215, "loss": 95.62724304199219, "time": 119}
+{"epoch": 4, "step": 220, "loss": 146.6149444580078, "time": 119}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 5}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 5}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 5}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 6}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 6}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 6}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 7}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 7}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 5}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 5}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 6}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 6}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 7}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 7}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 7}
+train_translation.py --load 0
+{"epoch": 0, "step": 0, "loss": 7.142178058624268, "time": 4}
+{"epoch": 0, "step": 5, "loss": 69.92982482910156, "time": 5}
+{"epoch": 0, "step": 10, "loss": 187.95425415039062, "time": 5}
+{"epoch": 0, "step": 15, "loss": 116.46453094482422, "time": 5}
+{"epoch": 0, "step": 20, "loss": 103.49996948242188, "time": 6}
+{"epoch": 0, "step": 25, "loss": 109.99765014648438, "time": 6}
+{"epoch": 0, "step": 30, "loss": 82.7474365234375, "time": 6}
+{"epoch": 0, "step": 35, "loss": 81.3102798461914, "time": 7}
+{"epoch": 0, "step": 40, "loss": 68.49085235595703, "time": 7}
+{"epoch": 1, "step": 45, "loss": 83.40009307861328, "time": 55}
+{"epoch": 1, "step": 50, "loss": 83.36439514160156, "time": 55}
+{"epoch": 1, "step": 55, "loss": 117.81816101074219, "time": 56}
+{"epoch": 1, "step": 60, "loss": 70.09979248046875, "time": 56}
+{"epoch": 1, "step": 65, "loss": 90.87323760986328, "time": 57}
+{"epoch": 1, "step": 70, "loss": 60.27517318725586, "time": 57}
+{"epoch": 1, "step": 75, "loss": 99.74661254882812, "time": 57}
+{"epoch": 1, "step": 80, "loss": 76.57121276855469, "time": 58}
+{"epoch": 1, "step": 85, "loss": 85.32162475585938, "time": 58}
+{"epoch": 2, "step": 90, "loss": 79.57125091552734, "time": 104}
+{"epoch": 2, "step": 95, "loss": 145.4536590576172, "time": 104}
+{"epoch": 2, "step": 100, "loss": 72.27653503417969, "time": 105}
+{"epoch": 2, "step": 105, "loss": 90.55571746826172, "time": 105}
+{"epoch": 2, "step": 110, "loss": 83.55565643310547, "time": 105}
+{"epoch": 2, "step": 115, "loss": 61.579551696777344, "time": 106}
+{"epoch": 2, "step": 120, "loss": 98.33128356933594, "time": 107}
+{"epoch": 2, "step": 125, "loss": 128.28770446777344, "time": 107}
+{"epoch": 2, "step": 130, "loss": 82.06121063232422, "time": 108}
+{"epoch": 3, "step": 135, "loss": 78.25971221923828, "time": 128}
+{"epoch": 3, "step": 140, "loss": 75.09734344482422, "time": 128}
+{"epoch": 3, "step": 145, "loss": 109.36125183105469, "time": 128}
+{"epoch": 3, "step": 150, "loss": 102.68833923339844, "time": 129}
+{"epoch": 3, "step": 155, "loss": 102.20543670654297, "time": 129}
+{"epoch": 3, "step": 160, "loss": 98.07948303222656, "time": 129}
+{"epoch": 3, "step": 165, "loss": 99.76647186279297, "time": 130}
+{"epoch": 3, "step": 170, "loss": 98.70307159423828, "time": 130}
+{"epoch": 3, "step": 175, "loss": 102.44486999511719, "time": 131}
+{"epoch": 4, "step": 180, "loss": 101.29882049560547, "time": 150}
+{"epoch": 4, "step": 185, "loss": 113.0394287109375, "time": 150}
+{"epoch": 4, "step": 190, "loss": 102.2679214477539, "time": 150}
+{"epoch": 4, "step": 195, "loss": 88.9566650390625, "time": 151}
+{"epoch": 4, "step": 200, "loss": 80.84623718261719, "time": 151}
+{"epoch": 4, "step": 205, "loss": 173.88238525390625, "time": 151}
+{"epoch": 4, "step": 210, "loss": 138.01107788085938, "time": 152}
+{"epoch": 4, "step": 215, "loss": 116.2401351928711, "time": 152}
+{"epoch": 4, "step": 220, "loss": 119.53892517089844, "time": 153}
+train_translation.py --load 0 --test_translation 1
+train_translation.py --load 0 --test_translation 1
+train_translation.py
+{"epoch": 0, "step": 0, "loss": 7.122797966003418, "time": 4}
+{"epoch": 0, "step": 5, "loss": 198.62460327148438, "time": 4}
+{"epoch": 0, "step": 10, "loss": 119.2225341796875, "time": 4}
+{"epoch": 0, "step": 15, "loss": 89.00521850585938, "time": 4}
+{"epoch": 0, "step": 20, "loss": 144.1955108642578, "time": 5}
+{"epoch": 0, "step": 25, "loss": 126.2806396484375, "time": 5}
+{"epoch": 0, "step": 30, "loss": 115.61041259765625, "time": 5}
+{"epoch": 0, "step": 35, "loss": 84.10115814208984, "time": 5}
+{"epoch": 0, "step": 40, "loss": 65.00213623046875, "time": 5}
+{"epoch": 1, "step": 45, "loss": 79.53411865234375, "time": 75}
+{"epoch": 1, "step": 50, "loss": 81.8320541381836, "time": 75}
+{"epoch": 1, "step": 55, "loss": 97.07718658447266, "time": 75}
+{"epoch": 1, "step": 60, "loss": 77.87088012695312, "time": 75}
+{"epoch": 1, "step": 65, "loss": 91.45843505859375, "time": 75}
+{"epoch": 1, "step": 70, "loss": 81.77067565917969, "time": 76}
+{"epoch": 1, "step": 75, "loss": 93.20482635498047, "time": 76}
+{"epoch": 1, "step": 80, "loss": 96.80836486816406, "time": 76}
+{"epoch": 1, "step": 85, "loss": 99.4000473022461, "time": 76}
+{"epoch": 2, "step": 90, "loss": 84.4419174194336, "time": 95}
+{"epoch": 2, "step": 95, "loss": 89.35089111328125, "time": 95}
+{"epoch": 2, "step": 100, "loss": 70.36296081542969, "time": 96}
+{"epoch": 2, "step": 105, "loss": 93.40479278564453, "time": 96}
+{"epoch": 2, "step": 110, "loss": 85.92987823486328, "time": 96}
+{"epoch": 2, "step": 115, "loss": 84.89830780029297, "time": 96}
+{"epoch": 2, "step": 120, "loss": 88.87590789794922, "time": 96}
+{"epoch": 2, "step": 125, "loss": 89.31674194335938, "time": 96}
+{"epoch": 2, "step": 130, "loss": 114.93965911865234, "time": 97}
+{"epoch": 3, "step": 135, "loss": 76.80366516113281, "time": 115}
+{"epoch": 3, "step": 140, "loss": 140.8549346923828, "time": 115}
+{"epoch": 3, "step": 145, "loss": 113.339111328125, "time": 116}
+{"epoch": 3, "step": 150, "loss": 93.06966400146484, "time": 116}
+{"epoch": 3, "step": 155, "loss": 113.3215103149414, "time": 116}
+{"epoch": 3, "step": 160, "loss": 109.3653335571289, "time": 116}
+{"epoch": 3, "step": 165, "loss": 139.5435333251953, "time": 116}
+{"epoch": 3, "step": 170, "loss": 76.41168975830078, "time": 117}
+{"epoch": 3, "step": 175, "loss": 132.55953979492188, "time": 117}
+{"epoch": 4, "step": 180, "loss": 109.78890228271484, "time": 143}
+{"epoch": 4, "step": 185, "loss": 88.3539810180664, "time": 143}
+{"epoch": 4, "step": 190, "loss": 113.5445327758789, "time": 144}
+{"epoch": 4, "step": 195, "loss": 107.1954345703125, "time": 144}
+{"epoch": 4, "step": 200, "loss": 127.9149398803711, "time": 144}
+{"epoch": 4, "step": 205, "loss": 131.3365936279297, "time": 144}
+{"epoch": 4, "step": 210, "loss": 129.23558044433594, "time": 145}
+{"epoch": 4, "step": 215, "loss": 86.24095153808594, "time": 145}
+{"epoch": 4, "step": 220, "loss": 143.04344177246094, "time": 145}
+barlow.py
+{"epoch": 0, "step": 0, "lr_weights": 0.0, "lr_biases": 0.0, "loss": 679.4036254882812, "time": 10}
+barlow.py
+{"epoch": 0, "step": 0, "lr_weights": 0.0, "lr_biases": 0.0, "loss": 456.90240478515625, "time": 8}
+barlow.py
+{"epoch": 0, "step": 0, "lr_weights": 0.0, "lr_biases": 0.0, "loss": 456.90240478515625, "time": 8}
+train_translation.py --batch_size=128 --dfeedforward=1024 --epochs=24 --nhead=4 --nlayers=4
+{"epoch": 0, "step": 0, "loss": 7.140841484069824, "time": 5}
+{"epoch": 2, "step": 5, "loss": 253.87469482421875, "time": 74}
+{"epoch": 5, "step": 10, "loss": 150.13229370117188, "time": 139}
+{"epoch": 7, "step": 15, "loss": 106.13131713867188, "time": 216}
+{"epoch": 10, "step": 20, "loss": 77.7083511352539, "time": 285}
+{"epoch": 12, "step": 25, "loss": 74.31400299072266, "time": 365}
+{"epoch": 15, "step": 30, "loss": 74.50468444824219, "time": 432}
+{"epoch": 17, "step": 35, "loss": 62.94711685180664, "time": 515}
+{"epoch": 20, "step": 40, "loss": 59.828826904296875, "time": 583}
+{"epoch": 22, "step": 45, "loss": 62.49226379394531, "time": 663}
+train_translation.py --batch_size=32 --dfeedforward=1024 --epochs=40 --nhead=4 --nlayers=6
+{"epoch": 0, "step": 0, "loss": 7.117185592651367, "time": 5}
+{"epoch": 0, "step": 5, "loss": 240.16217041015625, "time": 6}
+{"epoch": 1, "step": 10, "loss": 155.1521453857422, "time": 76}
+{"epoch": 2, "step": 15, "loss": 137.45753479003906, "time": 101}
+{"epoch": 3, "step": 20, "loss": 117.7391357421875, "time": 127}
+{"epoch": 4, "step": 25, "loss": 71.79619598388672, "time": 154}
+{"epoch": 5, "step": 30, "loss": 74.55005645751953, "time": 182}
+{"epoch": 5, "step": 35, "loss": 71.86864471435547, "time": 183}
+{"epoch": 6, "step": 40, "loss": 67.3455810546875, "time": 253}
+{"epoch": 7, "step": 45, "loss": 85.43989562988281, "time": 279}
+{"epoch": 8, "step": 50, "loss": 85.58329772949219, "time": 305}
+{"epoch": 9, "step": 55, "loss": 75.13690948486328, "time": 333}
+{"epoch": 10, "step": 60, "loss": 99.44623565673828, "time": 361}
+{"epoch": 10, "step": 65, "loss": 92.4845962524414, "time": 362}
+{"epoch": 11, "step": 70, "loss": 70.49784851074219, "time": 435}
+{"epoch": 12, "step": 75, "loss": 106.4268569946289, "time": 458}
+{"epoch": 13, "step": 80, "loss": 66.5932388305664, "time": 487}
+{"epoch": 14, "step": 85, "loss": 88.70879364013672, "time": 511}
+{"epoch": 15, "step": 90, "loss": 81.76454162597656, "time": 535}
+{"epoch": 15, "step": 95, "loss": 56.718807220458984, "time": 536}
+{"epoch": 16, "step": 100, "loss": 73.56828308105469, "time": 599}
+{"epoch": 17, "step": 105, "loss": 87.1954116821289, "time": 623}
+{"epoch": 18, "step": 110, "loss": 81.27310180664062, "time": 649}
+{"epoch": 19, "step": 115, "loss": 118.82411193847656, "time": 673}
+{"epoch": 20, "step": 120, "loss": 104.59524536132812, "time": 699}
+{"epoch": 20, "step": 125, "loss": 91.45010375976562, "time": 701}
+{"epoch": 21, "step": 130, "loss": 96.45476531982422, "time": 768}
+{"epoch": 22, "step": 135, "loss": 73.63231658935547, "time": 792}
+{"epoch": 23, "step": 140, "loss": 81.41030883789062, "time": 820}
+{"epoch": 24, "step": 145, "loss": 68.5522232055664, "time": 845}
+{"epoch": 25, "step": 150, "loss": 87.08369445800781, "time": 877}
+{"epoch": 25, "step": 155, "loss": 60.33863830566406, "time": 878}
+{"epoch": 26, "step": 160, "loss": 90.980224609375, "time": 943}
+{"epoch": 27, "step": 165, "loss": 89.83417510986328, "time": 967}
+{"epoch": 28, "step": 170, "loss": 59.04204177856445, "time": 995}
+{"epoch": 29, "step": 175, "loss": 76.57648468017578, "time": 1020}
+{"epoch": 30, "step": 180, "loss": 79.04066467285156, "time": 1047}
+{"epoch": 30, "step": 185, "loss": 116.04915618896484, "time": 1048}
+{"epoch": 31, "step": 190, "loss": 96.91857147216797, "time": 1120}
+{"epoch": 32, "step": 195, "loss": 117.3604965209961, "time": 1142}
+{"epoch": 33, "step": 200, "loss": 79.40359497070312, "time": 1173}
+{"epoch": 34, "step": 205, "loss": 118.38796997070312, "time": 1199}
+{"epoch": 35, "step": 210, "loss": 100.85802459716797, "time": 1227}
+{"epoch": 35, "step": 215, "loss": 127.6283187866211, "time": 1228}
+{"epoch": 36, "step": 220, "loss": 107.0147705078125, "time": 1295}
+{"epoch": 37, "step": 225, "loss": 101.71541595458984, "time": 1319}
+{"epoch": 38, "step": 230, "loss": 109.91344451904297, "time": 1354}
+{"epoch": 39, "step": 235, "loss": 91.43553924560547, "time": 1382}
diff --git a/sweep.yaml b/sweep.yaml
index 6402430..ae76056 100644
--- a/sweep.yaml
+++ b/sweep.yaml
@@ -1,17 +1,20 @@
-program: main.py
+program: train_translation.py
 method: bayes
 metric: 
     name: epoch_loss
     goal: minimize
 
-description: 'trial2 learning q distributions' 
+description: 'translation sweep' 
 parameters: 
 
-    lambd: 
+    epochs:
         distribution: 'q_uniform'
-        min: 0
-        max: 1
-        q: 0.05
+        min: 10
+        max: 40
+        q: 4
+
+    batch_size: 
+        values: [16, 32, 64, 128, 256]
     
     nhead:
         distribution: 'q_uniform'
@@ -19,6 +22,9 @@ parameters:
         max: 8
         q: 2
 
+    dfeedforward:
+        values: [ 256, 512, 1024]
+
     nlayers:
         distribution: 'q_uniform'
         min: 2
@@ -26,6 +32,6 @@ parameters:
         q: 2
 
     
-
+ # to add: lr, dropout, betas, loss_fn  
 
 
diff --git a/test_translation.py b/test_translation.py
index 67aad1e..47a6ecd 100644
--- a/test_translation.py
+++ b/test_translation.py
@@ -5,13 +5,20 @@ import os
 
 
 # translation pretraining 
+# sweep translation 
+# wandb sweep_translation.yaml 
 os.system('python ~/context_enhancement/context_enhancement/train_translation.py --load 0')
 
 # context enhancement
+# sweep barlow with translation encoder hyper-params 
+# sweep sweep_barlow.yaml
 os.system('python ~/context_enhancement/context_enhancement/barlow.py --load 1') 
 
 # tranining translation
+#train translation  with translation hyper-params
+#python train_translation.py 
 os.system('python ~/context_enhancement/context_enhancement/train_translation.py --load 1')
 
 # testing translation
+# no need
 os.system('python ~/context_enhancement/context_enhancement/train_translation.py --load 0')
diff --git a/train_translation.py b/train_translation.py
index 1b0fe42..f284015 100644
--- a/train_translation.py
+++ b/train_translation.py
@@ -101,6 +101,8 @@ parser.add_argument('--train', default=True , type=bool,
 parser.add_argument('--print_freq', default=5 , type=int,
                     metavar='PF', help='frequency of printing and saving stats')
 
+parser.add_argument('--test_translation', default=0, type=int, 
+                    metavar='TT', help='testing translation_score')
 ''' NOTE: 
         Transformer and tokenizer arguments would remain constant in training and context enhancement step.  
 '''
@@ -143,9 +145,9 @@ def main_worker(gpu, args):
 
     if args.rank == 0:
 
-#        wandb.init(config=args, project='translation_test')#############################################
-#        wandb.config.update(args)
-#        config = wandb.config
+        wandb.init(config=args, project='translation_test')#############################################
+        wandb.config.update(args)
+        config = wandb.config
     
         # exit()
         args.checkpoint_dir.mkdir(parents=True, exist_ok=True)
@@ -236,84 +238,101 @@ def main_worker(gpu, args):
     start_time = time.time()
 
 
-    
-    for epoch in range(start_epoch, args.epochs):
-        sampler.set_epoch(epoch)
-        epoch_loss = 0 
-        for step, (sent) in enumerate(loader, start=epoch * len(loader)):
-            src = sent[0].cuda(gpu, non_blocking=True)
-            tgt_inp = sent[2].cuda(gpu, non_blocking=True)
-            tgt_out = sent[3].cuda(gpu, non_blocking=True)
-            
-            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_inp, pad_idx) 
-            logits = model(src, tgt_inp, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)
-            
-            optimizer.zero_grad()
+    if not args.test_translation: 
 
-            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
-            loss.backward()
+        for epoch in range(start_epoch, args.epochs):
+            sampler.set_epoch(epoch)
+            epoch_loss = 0 
+            for step, (sent) in enumerate(loader, start=epoch * len(loader)):
+                src = sent[0].cuda(gpu, non_blocking=True)
+                tgt_inp = sent[2].cuda(gpu, non_blocking=True)
+                tgt_out = sent[3].cuda(gpu, non_blocking=True)
+                
+                src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_inp, pad_idx) 
+                logits = model(src, tgt_inp, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)
+                
+                optimizer.zero_grad()
+
+                loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
+                loss.backward()
 
-            optimizer.step()
-            # losses += loss.item()
+                optimizer.step()
+                # losses += loss.item()
+                
+                # wandb.log({'iter_loss': loss})
+                epoch_loss += loss.item()
+                torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
+                
+                if step % args.print_freq == 0:
+                    if args.rank == 0:
+                        stats = dict(epoch=epoch, step=step,
+                                    loss=loss.item(),
+                                    time=int(time.time() - start_time))
+                        print(json.dumps(stats))
+                        print(json.dumps(stats), file=stats_file)
+            if args.rank == 0:
+
+                wandb.log({"epoch_loss":epoch_loss})
+                # save checkpoint
+                state = dict(epoch=epoch + 1, model=model.module.state_dict(),
+                            optimizer=optimizer.state_dict())
+                # print(model.state_dict)
+                torch.save(state, args.checkpoint_dir / 'translation_checkpoint.pth')
+                print('translation model saved in', args.checkpoint_dir)
             
-#            wandb.log({'iter_loss': loss})
-            epoch_loss += loss.item()
-            torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
+    ##############################################################
+            if args.rank == 0: 
+                if epoch%args.checkbleu ==0 : 
+
+                    bleu_score = checkbleu(model, tokenizer, test_loader, gpu)
+                    wandb.log({'bleu_score': bleu_score}) 
+    #            print(bleu_score(predicted, target))
+    ##############################################################
+    #        if epoch%1 ==0 : 
+    #            torch.save(model.module.state_dict(),
+    #                   'path.pth')
+    #            print("Model is saved")
+            # if args.rank == 0:
+            #     # save checkpoint
+            #     state = dict(epoch=epoch + 1, model=model.state_dict(),
+            #                  optimizer=optimizer.state_dict())
+            #     torch.save(state, args.checkpoint_dir / f'translation_checkpoint.pth')
+            #     print('saved translation model in', args.checkpoint_dir)
+        wandb.finish()
             
-            if step % args.print_freq == 0:
-                if args.rank == 0:
-                    stats = dict(epoch=epoch, step=step,
-                                 loss=loss.item(),
-                                 time=int(time.time() - start_time))
-                    print(json.dumps(stats))
-                    print(json.dumps(stats), file=stats_file)
-        # wandb.log({"epoch_loss":epoch_loss})
-        if args.rank == 0:
-            # save checkpoint
-            state = dict(epoch=epoch + 1, model=model.module.state_dict(),
-                         optimizer=optimizer.state_dict())
-            # print(model.state_dict)
-            torch.save(state, args.checkpoint_dir / 'translation_checkpoint.pth')
-            print('translation model saved in', args.checkpoint_dir)
-        
-##############################################################
-        if epoch%args.checkbleu ==0 : 
+    else: 
+
+        bleu_score = checkbleu(model,tokenizer, test_loader, gpu )
+        print('test_bleu_score', bleu_score)
+        if args.rank == 0: 
+            wandb.log({'bleu_score': bleu_score})
+
 
-            model.eval()
-            predicted=[]
-            target=[]
+def checkbleu(model, tokenizer, test_loader, gpu): 
+
+    model.eval()
+    predicted=[]
+    target=[]
             
-            for i in test_loader: 
-                src = i[0].cuda(gpu, non_blocking=True)
-                tgt_out = i[3].cuda(gpu, non_blocking=True)
-                num_tokens = src.shape[0]
-
-                src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).cuda(gpu, non_blocking=True)
-                out = translate(model, src, tokenizer, src_mask, gpu)
-                predicted.append(out)
-                target.append([tokenizer.convert_ids_to_tokens(tgt_out)])
+    for i in test_loader: 
+        src = i[0].cuda(gpu, non_blocking=True)
+        tgt_out = i[3].cuda(gpu, non_blocking=True)
+        num_tokens = src.shape[0]
+
+        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool).cuda(gpu, non_blocking=True)
+        out = translate(model, src, tokenizer, src_mask, gpu)
+        predicted.append(out)
+        target.append([tokenizer.convert_ids_to_tokens(tgt_out)])
                 
-                try: 
-                    bleu_score(predicted, target)
-                except: 
-                    predicted.pop()
-                    target.pop()
+        try: 
+            bleu_score(predicted, target)
+        except: 
+            predicted.pop()
+            target.pop()
             
-            print(bleu_score(predicted, target))
-##############################################################
-#        if epoch%1 ==0 : 
-#            torch.save(model.module.state_dict(),
-#                   'path.pth')
-#            print("Model is saved")
-        # if args.rank == 0:
-        #     # save checkpoint
-        #     state = dict(epoch=epoch + 1, model=model.state_dict(),
-        #                  optimizer=optimizer.state_dict())
-        #     torch.save(state, args.checkpoint_dir / f'translation_checkpoint.pth')
-        #     print('saved translation model in', args.checkpoint_dir)
-#    wandb.finish()
-           
+        bleu = bleu_score(predicted, target)
 
+    return bleu
 
 '''
 todo: 
@@ -360,3 +379,4 @@ def translate(model: torch.nn.Module,
 
 if __name__ == '__main__': 
     main()
+    wandb.finish()
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index b8c8383..6163657 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20220406_171518-s7zesus8/logs/debug-internal.log
\ No newline at end of file
+run-20220409_182749-paufev36/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index 1d77d77..7d0f5dd 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20220406_171518-s7zesus8/logs/debug.log
\ No newline at end of file
+run-20220409_182749-paufev36/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index ad4b017..f11d588 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20220406_171518-s7zesus8
\ No newline at end of file
+run-20220409_182749-paufev36
\ No newline at end of file
