
train_translation.py --load=0
Reusing dataset opus_rf (/home/ivlabs/.cache/huggingface/datasets/opus_rf/de-en/1.0.0/3725eb23f8df679ddd37d8d65a6bbfcda7732c66edccbc62a3c3b1354c934c9f)
Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{"epoch": 0, "step": 0, "loss": 7.128603458404541, "time": 5}
{"epoch": 0, "step": 5, "loss": 156.04449462890625, "time": 6}
{"epoch": 0, "step": 10, "loss": 154.7353515625, "time": 7}
/home/ivlabs/context_enhancement/context_new/new/context_enhancement/train_translation.py:275: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)
translation model saved in checkpoint
{"epoch": 1, "step": 15, "loss": 138.67442321777344, "time": 73}
{"epoch": 1, "step": 20, "loss": 75.6456298828125, "time": 74}
translation model saved in checkpoint
{"epoch": 2, "step": 25, "loss": 64.19247436523438, "time": 92}
{"epoch": 2, "step": 30, "loss": 65.62056732177734, "time": 93}
{"epoch": 2, "step": 35, "loss": 66.36638641357422, "time": 93}
translation model saved in checkpoint
{"epoch": 3, "step": 40, "loss": 77.29269409179688, "time": 110}
{"epoch": 3, "step": 45, "loss": 68.74011993408203, "time": 111}
translation model saved in checkpoint
{"epoch": 4, "step": 50, "loss": 74.82659912109375, "time": 131}
{"epoch": 4, "step": 55, "loss": 77.39452362060547, "time": 132}
translation model saved in checkpoint
{"epoch": 5, "step": 60, "loss": 62.27414321899414, "time": 149}
{"epoch": 5, "step": 65, "loss": 90.9207992553711, "time": 150}
{"epoch": 5, "step": 70, "loss": 66.96754455566406, "time": 150}
translation model saved in checkpoint
{"epoch": 6, "step": 75, "loss": 71.40245819091797, "time": 216}
{"epoch": 6, "step": 80, "loss": 63.940818786621094, "time": 217}
translation model saved in checkpoint
{"epoch": 7, "step": 85, "loss": 50.857147216796875, "time": 233}
{"epoch": 7, "step": 90, "loss": 78.37335205078125, "time": 234}
{"epoch": 7, "step": 95, "loss": 100.13611602783203, "time": 234}
translation model saved in checkpoint
{"epoch": 8, "step": 100, "loss": 80.35195922851562, "time": 252}
{"epoch": 8, "step": 105, "loss": 86.00081634521484, "time": 253}
translation model saved in checkpoint
{"epoch": 9, "step": 110, "loss": 82.35330200195312, "time": 272}
{"epoch": 9, "step": 115, "loss": 88.81517791748047, "time": 273}
translation model saved in checkpoint