diff --git a/__pycache__/translation_dataset.cpython-37.pyc b/__pycache__/translation_dataset.cpython-37.pyc
index 3e1099c..38aec79 100644
Binary files a/__pycache__/translation_dataset.cpython-37.pyc and b/__pycache__/translation_dataset.cpython-37.pyc differ
diff --git a/__pycache__/translation_utils.cpython-37.pyc b/__pycache__/translation_utils.cpython-37.pyc
index 0bfa504..ba5fa6d 100644
Binary files a/__pycache__/translation_utils.cpython-37.pyc and b/__pycache__/translation_utils.cpython-37.pyc differ
diff --git a/barlow.py b/barlow.py
index e256395..9bb38cd 100644
--- a/barlow.py
+++ b/barlow.py
@@ -299,12 +299,12 @@ class BarlowTwins(nn.Module):
         x = x.squeeze(-1)
         #print(x.shape)
         x = self.mbert(x)
-        x = self.transformer_enc(x["last_hidden_state"]).permute(1, 0, 2)) 
+        x = self.transformer_enc(x["last_hidden_state"]).permute(1, 0, 2)
         x = torch.sum(x, dim=0)/x.shape[1] # using avg pooling 
         
         y = y.squeeze(-1)
         y = self.mbert(y)
-        y = self.transformer_enc(y["last_hidden_state"]).permute(1, 0, 2)) 
+        y = self.transformer_enc(y["last_hidden_state"]).permute(1, 0, 2) 
         y = torch.sum(y, dim=0)/y.shape[1] # using avg pooling 
 
         x = self.projector(x) #x = [batch_size, projector]
diff --git a/train_translation.py b/train_translation.py
index 58896e6..11103e5 100644
--- a/train_translation.py
+++ b/train_translation.py
@@ -26,7 +26,7 @@ from torch import Tensor
 # from torchtext.metrics import bleu_score
 import wandb 
 
-import barlow
+#import barlow
 
 os.environ['WANDB_START_METHOD'] = 'thread'
 
@@ -120,7 +120,7 @@ def main_worker(gpu, args):
     
     args.rank += gpu
     torch.distributed.init_process_group(
-        backend='gloo', init_method=args.dist_url,
+        backend='nccl', init_method=args.dist_url,
         world_size=args.world_size, rank=args.rank)
 
     if args.rank == 0:
@@ -149,7 +149,7 @@ def main_worker(gpu, args):
 #    transformer1 = nn.TransformerEncoderLayer(d_model = args.dmodel, nhead=args.nhead, dim_feedforward=args.dfeedforward, batch_first=True)
     # t_enc = nn.TransformerEncoder(transformer1, num_layers=args.nlayers)
     model = Translator(src_vocab_size = src_vocab_size, tgt_vocab_size=trg_vocab_size).cuda(gpu)
-    model_barlow = barlow.BarlowTwins(projector_layers=args.projector, mbert_out_size=args.mbert_out_size, transformer_enc=model.transformer.encoder, lambd=args.lambd).cuda(gpu)
+#    model_barlow = barlow.BarlowTwins(projector_layers=args.projector, mbert_out_size=args.mbert_out_size, transformer_enc=model.transformer.encoder, lambd=args.lambd).cuda(gpu)
     '''
     to_do: 
     if post_train: 
@@ -171,6 +171,7 @@ def main_worker(gpu, args):
 
 ###########################################################
     optimizer =torch.optim.Adam(model.parameters(), lr=args.learning_rate, betas=args.betas, eps=args.eps) 
+    
     if args.loss_fn == 'cross_entropy': 
         loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_idx)
 ##############################################################
@@ -185,6 +186,9 @@ def main_worker(gpu, args):
     loader = torch.utils.data.DataLoader(
          dataset, batch_size=per_device_batch_size, num_workers=args.workers,
          pin_memory=True, sampler=sampler, collate_fn = MyCollate(tokenizer=tokenizer,bert2id_dict=dataset.bert2id_dict))
+    test_loader = torch.utils.data.DataLoader(
+         dataset, batch_size=1, num_workers=args.workers,
+         pin_memory=True, sampler=sampler, collate_fn = MyCollate(tokenizer=tokenizer,bert2id_dict=dataset.bert2id_dict))
     #############################
     start_time = time.time()
 
@@ -220,6 +224,30 @@ def main_worker(gpu, args):
                     print(json.dumps(stats))
                     print(json.dumps(stats), file=stats_file)
         wandb.log({"epoch_loss":epoch_loss})
+        
+##############################################################
+        if epoch%1 ==0 : 
+
+            model.eval()
+            predicted=[]
+            target=[]
+            
+            for i in test_loader: 
+                src = i[0].cuda(gpu, non_blocking=True)
+                tgt_out = i[1].cuda(gpu, non_blocking=True)
+                out = translate(model, src, tokenizer)
+                predicted.append(out)
+                target.append([tokenizer.convert_ids_to_tokens(tgt_out)])
+                
+                try: 
+                    bleu_score(predicted, target)
+                except: 
+                    predicted.pop()
+                    target.pop()
+            
+            print(bleu_score(predicted, target))
+##############################################################
+
         if args.rank == 0:
             # save checkpoint
             state = dict(epoch=epoch + 1, model=model.state_dict(),
@@ -289,15 +317,15 @@ todo:
 
 # function to generate output sequence using greedy algorithm 
 def greedy_decode(model, src, src_mask, max_len, start_symbol, eos_idx):
-    src = src.cuda(gpu, non_blocking=True)
-    src_mask = src_mask.cuda(gpu, non_blocking=True)
+    src = src
+    src_mask = src_mask
 
     memory = model.encode(src, src_mask)
-    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).cuda(gpu, non_blocking=True)
+    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long)
     for i in range(max_len-1):
         memory = memory.cuda(gpu, non_blocking=True)
         tgt_mask = (translation_utils.generate_square_subsequent_mask(ys.size(0))
-                    .type(torch.bool)).cuda(gpu, non_blocking=True)
+                    .type(torch.bool))
         out = model.decode(ys, memory, tgt_mask)
         out = out.transpose(0, 1)
         prob = model.generator(out[:, -1])
@@ -322,5 +350,6 @@ def translate(model: torch.nn.Module,
         model,  src, src_mask, max_len=num_tokens + 5, start_symbol=tokenizer.cls_token, eos_idx=tokenizer.sep_token).flatten()
     return tokenizer.convert_ids_to_tokens(tgt_tokens) 
 
+
 if __name__ == '__main__': 
     main()
